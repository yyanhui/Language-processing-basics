{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008cb64e-dc3c-4bf0-88c1-2639681fe897",
   "metadata": {},
   "source": [
    "## Attention\n",
    "The distribution between sentence parts. The rnn structure depend highly on the exact previous text, to emphasize the connection between context, we use a hidden state, attention.\n",
    "\n",
    "### Calculation\n",
    "How's the attention layer calculated:\n",
    "\n",
    "1. Set query(Q), key(K), value(V) vecters for each embedded token. $Q = X * W_Q$, $K = X * W_K$, $V = X * W_V$\n",
    "2. for each token i, calculate the score $softmax((q_i * k_j)/\\sqrt(d_k)) * v_j$, and sum the results for all j. d_k lowers the scale of first score(usually the dimension of key vector), softmax normalizes them, v_j ensures the values we want to focus stay intact\n",
    "\n",
    "The calculation here in matrix form is $Z = SoftMax(\\frac{QK^T}{\\sqrt(d_k)})V$\n",
    "\n",
    "### Multi-head attention\n",
    "splits up the controller states into chunks and operates the self attention on each chunk separately and then recombines with a fully connected network.\n",
    "\n",
    "![Multi-heads attention](attention.png)\n",
    "\n",
    "The picture here is from https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "### Masked attention\n",
    "Attention shouldn't have access to text in time j at time i (if j > i), so the attentions reading from right to left are masked out with a casual masking matrix\n",
    "\n",
    "M_{casual} =\n",
    "\\begin{bmatrix}\n",
    "0 & -\\infty & -\\infty & -\\infty & -\\infty \\\\\n",
    "0 & 0 & -\\infty & -\\infty & -\\infty \\\\\n",
    "0 & 0 & 0 & -\\infty & -\\infty \\\\\n",
    "0 & 0 & 0 & 0 & -\\infty \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8256d3ed-52ea-4ae9-aeed-f6d7c417fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, bias, dropout_rate=0.0):\n",
    "  \"\"\"Dot-product attention.\n",
    "\n",
    "  Args:\n",
    "    q: Tensor with shape [..., length_q, depth_k].\n",
    "    k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must\n",
    "      match with q.\n",
    "    v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must\n",
    "      match with q.\n",
    "    bias: bias Tensor (see attention_bias())\n",
    "    dropout_rate: a float.\n",
    "\n",
    "  Returns:\n",
    "    Tensor with shape [..., length_q, depth_v].\n",
    "  \"\"\"\n",
    "  logits = tf.matmul(q, k, transpose_b=True)  # [..., length_q, length_kv]\n",
    "  logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1])))\n",
    "  if bias is not None:\n",
    "    # `attention_mask` = [B, T]\n",
    "    from_shape = get_shape_list(q)\n",
    "    if len(from_shape) == 4:\n",
    "      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32)\n",
    "    elif len(from_shape) == 5:\n",
    "      # from_shape = [B, N, Block_num, block_size, depth]#\n",
    "      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3],\n",
    "                                1], tf.float32)\n",
    "\n",
    "    bias = tf.matmul(broadcast_ones,\n",
    "                     tf.cast(bias, tf.float32), transpose_b=True)\n",
    "\n",
    "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "    # masked positions, this operation will create a tensor which is 0.0 for\n",
    "    # positions we want to attend and -10000.0 for masked positions.\n",
    "    adder = (1.0 - bias) * -10000.0\n",
    "\n",
    "    # Since we are adding it to the raw scores before the softmax, this is\n",
    "    # effectively the same as removing these entirely.\n",
    "    logits += adder\n",
    "  else:\n",
    "    adder = 0.0\n",
    "\n",
    "  attention_probs = tf.nn.softmax(logits, name=\"attention_probs\")\n",
    "  attention_probs = dropout(attention_probs, dropout_rate)\n",
    "  return tf.matmul(attention_probs, v)\n",
    "    \n",
    "def attention_layer(from_tensor,\n",
    "                    to_tensor,\n",
    "                    attention_mask=None,\n",
    "                    num_attention_heads=1,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.0,\n",
    "                    initializer_range=0.02,\n",
    "                    batch_size=None,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None,\n",
    "                    use_einsum=True):\n",
    "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
    "\n",
    "  Args:\n",
    "    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
    "      from_width].\n",
    "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
    "    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      The values should be 1 or 0. The attention scores will effectively\n",
    "      be set to -infinity for any positions in the mask that are 0, and\n",
    "      will be unchanged for positions that are 1.\n",
    "    num_attention_heads: int. Number of attention heads.\n",
    "    query_act: (optional) Activation function for the query transform.\n",
    "    key_act: (optional) Activation function for the key transform.\n",
    "    value_act: (optional) Activation function for the value transform.\n",
    "    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
    "      attention probabilities.\n",
    "    initializer_range: float. Range of the weight initializer.\n",
    "    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
    "      of the 3D version of the `from_tensor` and `to_tensor`.\n",
    "    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "      of the 3D version of the `from_tensor`.\n",
    "    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "      of the 3D version of the `to_tensor`.\n",
    "    use_einsum: bool. Whether to use einsum or reshape+matmul for dense layers\n",
    "\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, from_seq_length, num_attention_heads,\n",
    "      size_per_head].\n",
    "\n",
    "  Raises:\n",
    "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
    "  \"\"\"\n",
    "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
    "  size_per_head = int(from_shape[2]/num_attention_heads)\n",
    "\n",
    "  if len(from_shape) != len(to_shape):\n",
    "    raise ValueError(\n",
    "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
    "\n",
    "  if len(from_shape) == 3:\n",
    "    batch_size = from_shape[0]\n",
    "    from_seq_length = from_shape[1]\n",
    "    to_seq_length = to_shape[1]\n",
    "  elif len(from_shape) == 2:\n",
    "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
    "      raise ValueError(\n",
    "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
    "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
    "          \"must all be specified.\")\n",
    "\n",
    "  # Scalar dimensions referenced here:\n",
    "  #   B = batch size (number of sequences)\n",
    "  #   F = `from_tensor` sequence length\n",
    "  #   T = `to_tensor` sequence length\n",
    "  #   N = `num_attention_heads`\n",
    "  #   H = `size_per_head`\n",
    "\n",
    "  # `query_layer` = [B, F, N, H]\n",
    "  q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head,\n",
    "                     create_initializer(initializer_range), query_act,\n",
    "                     use_einsum, \"query\")\n",
    "\n",
    "  # `key_layer` = [B, T, N, H]\n",
    "  k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,\n",
    "                     create_initializer(initializer_range), key_act,\n",
    "                     use_einsum, \"key\")\n",
    "  # `value_layer` = [B, T, N, H]\n",
    "  v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,\n",
    "                     create_initializer(initializer_range), value_act,\n",
    "                     use_einsum, \"value\")\n",
    "  q = tf.transpose(q, [0, 2, 1, 3])\n",
    "  k = tf.transpose(k, [0, 2, 1, 3])\n",
    "  v = tf.transpose(v, [0, 2, 1, 3])\n",
    "  if attention_mask is not None:\n",
    "    attention_mask = tf.reshape(\n",
    "        attention_mask, [batch_size, 1, to_seq_length, 1])\n",
    "    # 'new_embeddings = [B, N, F, H]'\n",
    "  new_embeddings = dot_product_attention(q, k, v, attention_mask,\n",
    "                                         attention_probs_dropout_prob)\n",
    "\n",
    "  return tf.transpose(new_embeddings, [0, 2, 1, 3])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fce83-6ad3-4f24-843e-06af60b87c2e",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are a multi-layer neural attention model\n",
    "\n",
    "General transformer models: BERT, ALBERT, RoBERTa(robust optimized BERT), DistilBERT, ELECTRA, XLM / XLM-RoBERTa(cross-lingual), GPT, etc.\n",
    "\n",
    "![Bert](BERT.png)\n",
    "this picture is from https://huggingface.co/blog/bert-101#3-bert-model-size--architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e580605-1fd6-479b-b374-d598ade23277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
