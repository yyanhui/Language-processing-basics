{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa93cd65-569e-4290-8369-7c326cf064df",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Splitting the text into smaller units such as words or sub-word, to segment the unstructured data to tokens and subtract information from them.\n",
    "#### Reduces size of data\n",
    "#### Feature extraction and information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6277fd-eaa4-402c-a0dc-fe317a30f7b3",
   "metadata": {},
   "source": [
    "## Types of tokenization\n",
    "Words, Sub-words, Sentence, character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9c6d2c-31db-4e96-a4e2-91fc4383d9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello world, here's an illustration for tokenization with nltk!\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "text = \"Hello world, here's an illustration for tokenization with nltk!\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8dbf047-b87a-49d2-afe0-f9d3c0360d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " ',',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'an',\n",
       " 'illustration',\n",
       " 'for',\n",
       " 'tokenization',\n",
       " 'with',\n",
       " 'nltk',\n",
       " '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## words\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"Hello world, here's an illustration for tokenization with nltk!\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0884f325-6163-40b3-8206-3aaa33c2b69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " 'here',\n",
       " 's',\n",
       " 'an',\n",
       " 'illustration',\n",
       " 'for',\n",
       " 'tokenization',\n",
       " 'with',\n",
       " 'nltk']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## regular expression method\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    " \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text = \"Hello world, here's an illustration for tokenization with nltk!\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c42c367-1848-4e0b-9350-8c7db4ef8964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " ',',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'illustration',\n",
       " 'for',\n",
       " 'tokenization',\n",
       " 'with',\n",
       " 'nltk',\n",
       " '!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## wordPunctTokenizer: keep the punctuation\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    " \n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Hello world, here's an illustration for tokenization with nltk!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d8c47-2833-4e57-a305-c377dcf06fbd",
   "metadata": {},
   "source": [
    "### SpaCy: pretrained tokenizing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2408aa-4ffc-4a83-9c86-996e3a55775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Hello world, here's an illustration for tokenization with SpaCy!\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04166379-ca0c-497e-a0f2-f0511aa11a6a",
   "metadata": {},
   "source": [
    "### Bert Tokenizer\n",
    "WordPiece tokenizer, which splits words into subwords based on frequency. The frequent words are kept and rare words are splited into sub-words, effective in rare word analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009faa81-1a86-453e-bdfe-7d0b95da9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', ',', 'here', \"'\", 's', 'an', 'illustration', 'for', 'token', '##ization', 'with', 'bert', '##tok', '##eni', '##zer', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Hello world, here's an illustration for tokenization with BertTokenizer!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd00cb0-817f-4575-b364-a051fcc71420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 1010, 2182, 1005, 1055, 2019, 14614, 2005, 19204, 3989, 2007, 14324, 18715, 18595, 6290, 999]\n"
     ]
    }
   ],
   "source": [
    "##can be converted to ids\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e69e200-966f-4cc2-ba11-60a8ea2261f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  2088,  1010,  2182,  1005,  1055,  2019, 14614,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## when used in BERT models, padding and attention masks are required\n",
    "encoded = tokenizer(text, padding=\"max_length\", truncation=True, max_length=10, return_tensors=\"pt\")\n",
    "print(encoded[\"input_ids\"])  # Token IDs\n",
    "print(encoded[\"attention_mask\"])  # 1 for real tokens, 0 for padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e548b54-2965-427d-82be-e33511ee50b5",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding (BPE)\n",
    "sub-word tokenization algorithm, split text into characters first, then combine most frequent bigrams, repeat the process untill limit of vacabulary size reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e03a62e7-800f-4cdd-9b70-c4c29e13dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Ġworld', ',', 'Ġhere', \"'s\", 'Ġan', 'Ġillustration', 'Ġfor', 'Ġtoken', 'ization', 'Ġwith', 'ĠB', 'PE', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # GPT-2 uses BPE\n",
    "text = \"Hello world, here's an illustration for tokenization with BPE!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7956fde-bb4c-4675-a240-fa2d2d983742",
   "metadata": {},
   "source": [
    "### Sentence Piece\n",
    "sub-word algorithm based on BPE or unigram language models, but treats the input text as a raw stream of characters and remove spaces as word boundaries, works better in non-space-separated languages (Chinese, Japanese, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "188f8778-a83c-4f09-b532-ba1ab161d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: de-en.txt\n",
      "  input_format: \n",
      "  model_prefix: mymodel\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: de-en.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1917286), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1917286 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=322814382\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9565% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=78\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999565\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1917286 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=171000070\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1000078 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1917286\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 636628\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 636628 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=262286 obj=11.5537 num_tokens=1480006 num_tokens/piece=5.64272\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=192464 obj=8.56834 num_tokens=1484136 num_tokens/piece=7.71124\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=144182 obj=8.53139 num_tokens=1521601 num_tokens/piece=10.5533\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=143918 obj=8.52441 num_tokens=1521914 num_tokens/piece=10.5749\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=107917 obj=8.54047 num_tokens=1598786 num_tokens/piece=14.815\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=107903 obj=8.53847 num_tokens=1598825 num_tokens/piece=14.8172\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=80919 obj=8.56552 num_tokens=1688640 num_tokens/piece=20.8683\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=80916 obj=8.56284 num_tokens=1688832 num_tokens/piece=20.8714\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=60684 obj=8.60481 num_tokens=1793327 num_tokens/piece=29.5519\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=60681 obj=8.60143 num_tokens=1793295 num_tokens/piece=29.5528\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45507 obj=8.66115 num_tokens=1909712 num_tokens/piece=41.9652\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45507 obj=8.65248 num_tokens=1909647 num_tokens/piece=41.9638\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34129 obj=8.73865 num_tokens=2036594 num_tokens/piece=59.6734\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34127 obj=8.73085 num_tokens=2036564 num_tokens/piece=59.676\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25595 obj=8.84363 num_tokens=2179325 num_tokens/piece=85.1465\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25594 obj=8.83303 num_tokens=2179339 num_tokens/piece=85.1504\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19194 obj=8.98148 num_tokens=2329866 num_tokens/piece=121.385\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19194 obj=8.95582 num_tokens=2330030 num_tokens/piece=121.394\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14395 obj=9.15899 num_tokens=2496081 num_tokens/piece=173.399\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14395 obj=9.12097 num_tokens=2496129 num_tokens/piece=173.403\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10796 obj=9.37226 num_tokens=2669939 num_tokens/piece=247.308\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10796 obj=9.32831 num_tokens=2670057 num_tokens/piece=247.319\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=9.5417 num_tokens=2798452 num_tokens/piece=318.006\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=9.51506 num_tokens=2799060 num_tokens/piece=318.075\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: mymodel.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: mymodel.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train a tokenizer on a corpus\n",
    "spm.SentencePieceTrainer.train(input=\"de-en.txt\", model_prefix=\"mymodel\", vocab_size=8000)\n",
    "sp = spm.SentencePieceProcessor(model_file=\"mymodel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69f057dd-3da4-41a9-9d71-2ef9b991283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hel', 'lo', '▁wo', 'r', 'l', 'd', ',', '▁her', 'e', \"'\", 's', '▁an', '▁', 'ill', 'ust', 'r', 'ation', '▁', 'for', '▁', 'to', 'ken', 'iz', 'ation', '▁', 'wi', 'th', '▁B', 'P', 'E', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a sentence\n",
    "text = \"Hello world, here's an illustration for tokenization with BPE!\"\n",
    "tokens = sp.encode(text, out_type=str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "287b4367-162f-496d-9b1c-d3b68a3f87c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world, here's an illustration for tokenization with BPE!\n"
     ]
    }
   ],
   "source": [
    "# Decode into sentence\n",
    "decoded_text = sp.decode(tokens)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecee0e-455c-49dc-8509-8997826a33ba",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "reduce words to base or root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53e939b-2f76-4005-b86d-1d4bf24a03ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/xiao/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/xiao/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize a word (default POS is noun)\n",
    "print(lemmatizer.lemmatize(\"running\"))  # \"running\" → \"running\" (as noun by default)\n",
    "print(lemmatizer.lemmatize(\"running\", pos=wordnet.VERB))  # \"running\" → \"run\" (as verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c12c41-2964-4375-90fa-dc36459901f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You -> you\n",
      "'re -> be\n",
      "looking -> look\n",
      "at -> at\n",
      "illustrations -> illustration\n",
      "for -> for\n",
      "tokenization -> tokenization\n",
      "with -> with\n",
      "SpaCy -> SpaCy\n",
      "! -> !\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Sample sentence\n",
    "doc = nlp(\"You're looking at illustrations for tokenization with SpaCy!\")\n",
    "# Lemmatize each token in the sentence\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fe6ed-aaf1-4143-acc4-bb3a9aef8af9",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "reduces words to root by removing suffixes.Porter/Lancaster/Snowball/Lovis/Rule-based stemmer\n",
    "\n",
    "Porter stemmer is moderate with regard to agressiveness, only support English\n",
    "\n",
    "Lancaster stemmer is the most agressive, only support English\n",
    "\n",
    "Snowball is modified upon Porter, moderately agressive, supports over 30 languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794f06e3-e52f-4e8f-a5c5-473a71ceb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary data from NLTK (if needed)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"You're looking at illustrations for stemming with nltk!\"\n",
    "# Tokenize the sentence into words\n",
    "words = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c39a79f0-e78b-46fd-afe5-c264c42d2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['You', \"'re\", 'looking', 'at', 'illustrations', 'for', 'stemming', 'with', 'nltk', '!']\n",
      "Stemmed words: ['you', \"'re\", 'look', 'at', 'illustr', 'for', 'stem', 'with', 'nltk', '!']\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer object\n",
    "stemmer = PorterStemmer()\n",
    "# Stem each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original words: {words}\")\n",
    "print(f\"Stemmed words: {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69a497fc-c950-4942-bdd6-c4e82462155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['you', \"'re\", 'look', 'at', 'illust', 'for', 'stem', 'with', 'nltk', '!']\n"
     ]
    }
   ],
   "source": [
    "# LancasterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "stemmed_words = [lancaster_stemmer.stem(word) for word in words]\n",
    "print(f\"Stemmed words: {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a87f7337-6baa-4273-9fd6-942dff120c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['you', 're', 'look', 'at', 'illustr', 'for', 'stem', 'with', 'nltk', '!']\n"
     ]
    }
   ],
   "source": [
    "#SnowballStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "stemmed_words = [snowball_stemmer.stem(word) for word in words]\n",
    "print(f\"Stemmed words: {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b592532-7807-4e99-9ae7-8b86b26e56e5",
   "metadata": {},
   "source": [
    "# Stopword removal\n",
    "remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eba8b8b2-f5ef-4b5b-8b08-729a75cf26e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/xiao/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['example', 'sentence', 'demonstrate', 'stopword', 'removal', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the stopwords list (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence to demonstrate stopword removal.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Get the list of stopwords in English\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stopwords from the tokenized words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58500ea2-28f3-4bbe-b816-4823eeffd0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['example', 'sentence', 'demonstrate', 'stopword', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence to demonstrate stopword removal.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filter out stopwords from the tokens\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401bf7a-2762-491d-8b8c-d8cd197aad69",
   "metadata": {},
   "source": [
    "# Parts of speech tagging\n",
    "assigns a part of speech to each word in sentence based on definition and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c1e3f5f-4e22-496d-a8ea-2ed9b12bb500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You -> PRON\n",
      "'re -> AUX\n",
      "looking -> VERB\n",
      "at -> ADP\n",
      "illustrations -> NOUN\n",
      "for -> ADP\n",
      "POS -> PROPN\n",
      "tagging -> VERB\n",
      "with -> ADP\n",
      "Spacy -> PROPN\n",
      "! -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"You're looking at illustrations for POS tagging with Spacy!\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Display POS tags for each token\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a56c06b2-89ba-4948-b9e6-f4f875756a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/xiao/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You -> PRP\n",
      "'re -> VBP\n",
      "looking -> VBG\n",
      "at -> IN\n",
      "illustrations -> NNS\n",
      "for -> IN\n",
      "POS -> NNP\n",
      "tagging -> VBG\n",
      "with -> IN\n",
      "nltk -> NN\n",
      "! -> .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"You're looking at illustrations for POS tagging with nltk!\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Display POS tags\n",
    "for word, tag in tags:\n",
    "    print(f\"{word} -> {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec42e2a-f326-4d25-9ff0-b6a6c0766690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
